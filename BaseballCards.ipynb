{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Dependencies\n",
        "! pip install requests        # To get HTML file\n",
        "! pip install BeautifulSoup4  # For HTML parsing\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnUriXNkO0iN",
        "outputId": "525852ff-2ace-4169-e178-c86144954267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAGE_MAX = 300\n",
        "POP_URL_BASE = \"https://www.psacard.com/Pop/GetSetItems\"\n",
        "EXAMPLE_URL = \"https://www.psacard.com/pop/baseball-cards/1952/topps/49722\"\n",
        "\n",
        "SET_ID = \"49722\"\n",
        "\n",
        "def scrape(pop_url, set_name):\n",
        "    # Get json data for input set\n",
        "    sess = requests.Session()\n",
        "    sess.mount(\"https://\", requests.adapters.HTTPAdapter(max_retries=5))\n",
        "    form_data = {\n",
        "        \"headingID\": SET_ID,\n",
        "        \"categoryID\": \"20019\",\n",
        "        \"draw\": 1,\n",
        "        \"start\": 0,\n",
        "        \"length\": 500,\n",
        "        \"isPSADNA\": \"false\"\n",
        "    }\n",
        "\n",
        "    json_data = post_to_url(sess, form_data)\n",
        "    cards = json_data[\"data\"]\n",
        "\n",
        "    # If there's more than PAGE_MAX results, keep calling the scrape url until we have all of the card records\n",
        "    total_cards = json_data[\"recordsTotal\"]\n",
        "    if total_cards > PAGE_MAX:\n",
        "        additional_pages = math.ceil((total_cards - PAGE_MAX) / PAGE_MAX)\n",
        "        for i in range(additional_pages):\n",
        "            curr_page = i + 1\n",
        "            form_data = {\n",
        "                \"headingID\": SET_ID,\n",
        "                \"categoryID\": \"20019\",\n",
        "                \"draw\": curr_page + 2,\n",
        "                \"start\": PAGE_MAX * curr_page,\n",
        "                \"length\": PAGE_MAX,\n",
        "                \"isPSADNA\": \"false\"\n",
        "            }\n",
        "\n",
        "            json_data = post_to_url(sess, form_data)\n",
        "            cards += json_data[\"data\"]\n",
        "\n",
        "    # Create a dataframe\n",
        "    df = pd.DataFrame(cards[1:]).drop_duplicates()\n",
        "    save_df(df, set_name)\n",
        "\n",
        "# Output the dataframe as a file\n",
        "def save_df(df, set_name):\n",
        "    df.to_csv(get_file_name(set_name), index = False)\n",
        "    date = \"09_09_2023\"\n",
        "    drive.mount('/content/gdrive')\n",
        "    destination = '/content/gdrive/My Drive/BaseballCardsInfo/' + get_file_name(set_name)\n",
        "    print(destination)\n",
        "    df.to_csv(destination, index = False)\n",
        "\n",
        "def post_to_url(session, form_data):\n",
        "    r = session.post(POP_URL_BASE, data=form_data)\n",
        "    r.raise_for_status()\n",
        "    json_data = r.json()\n",
        "    time.sleep(3)\n",
        "    return json_data\n",
        "\n",
        "def get_file_name(set_name):\n",
        "    f_name = \"{}--{}\".format(set_name.replace(\" \", \"-\"), str(time.strftime(\"%Y-%m-%d-%H%M%S\")))\n",
        "    return \"{}.csv\".format(f_name)\n",
        "\n",
        "url = \"https://www.psacard.com/pop/baseball-cards/1952/topps/49722\"\n",
        "set_name = \"1952 Topps\"\n",
        "\n",
        "# Initialize class and execute web scraping\n",
        "ppr = scrape(url, set_name)"
      ],
      "metadata": {
        "id": "Yv5tlf4TwUyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83ed67b-fe5a-46c0-d462-ff0ad6735ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/BaseballCardsInfo/1952-Topps--2024-03-13-012742.csv\n"
          ]
        }
      ]
    }
  ]
}